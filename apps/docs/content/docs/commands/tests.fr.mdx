---
title: /setup-tests
description: Mettre en place une infrastructure de tests automatisés
---
# /setup-tests

Détectez automatiquement votre type de projet et démarrez une infrastructure de test complète avec l'installation du framework, la configuration, des exemples de tests et l'intégration CI.

## Démarrage rapide

```bash
/agileflow:setup-tests FRAMEWORK=auto COVERAGE=yes
```

## Objectif

Cette commande crée une base de tests professionnels en :
- Détection automatique du langage et du framework de votre projet
- Installation des dépendances du framework de test approprié
- Création de fichiers de configuration de tests
- Génération d'exemples de tests unitaires, d'intégration et E2E
- Ajout de scripts de test à votre configuration de build
- Intégration des tests dans le pipeline CI/CD
- Création de documentation de tests et de bonnes pratiques
- Exécution de tests pour vérifier que l'installation fonctionne

## Paramètres

| Paramètre | Obligatoire | Par défaut | Descriptif |
|-----------|----------|---------|-------------|
| `FRAMEWORK` | Non | `auto` | `auto`, `jest`, `mocha`, `pytest`, `rspec`, `go-test`, `cargo-test` |
| `COVERAGE` | Non | `yes` | Activer les rapports de couverture : `yes` ou `no` |
| `E2E` | Non | `no` | Inclure les tests E2E : `yes` ou `no` |

## Exemples

### Détection et configuration automatiques

```bash
/agileflow:setup-tests
```

Détecte Node.js/Python/Ruby/Go/Rust et installe le framework approprié.

### Cadre spécifique à la force

```bash
/agileflow:setup-tests FRAMEWORK=jest COVERAGE=yes
```

Utilise Jest pour les tests avec la couverture activée.

### Inclure les tests E2E

```bash
/agileflow:setup-tests E2E=yes
```

Configure les tests unitaires, d'intégration ET de bout en bout (Playwright pour les applications Web).

### Projet Python

```bash
/agileflow:setup-tests FRAMEWORK=pytest
```

Installe pytest avec couverture et crée des exemples de tests.

## Détection de projet

La commande détecte automatiquement votre pile technologique :

| Pile technologique | Cadre | Détecté par |
|-----------|-----------|-------------|
| Noeud.js | Blague | package.json |
| Node.js (plus ancien) | Moka | package.json |
| Python | pytest | exigences.txt, pyproject.toml |
| Rubis | RSpec | Fichier Gem |
| Aller | aller tester | aller.mod |
| Rouille | cargaison test | Cargo.toml |
| Java | JUnit | pom.xml, build.gradle |
| .NET | xUnit/NUnité | *.csproj |

## Fichiers de sortie

La commande crée une configuration de test complète :

### Fichiers de configuration

**Blague** (`jest.config.js`) :
```javascript
module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'node',
  coverageDirectory: 'coverage',
  collectCoverageFrom: [
    'src/**/*.{js,ts}',
    '!src/**/*.d.ts',
  ],
  coverageThreshold: {
    global: {
      branches: 70,
      functions: 70,
      lines: 70,
      statements: 70
    }
  }
};
```

**pytest** (`pytest.ini`) :
```ini
[pytest]
testpaths = tests
python_files = test_*.py *_test.py
addopts = --cov=src --cov-report=html --cov-report=term
```

**RSpec** (`.rspec`) :
```text
--format documentation
--require spec_helper
```

### Structure du répertoire

Crée des répertoires de tests organisés :

<FileTree>
  <Folder name="tests">
    <Folder name="unit" />
    <Folder name="integration" />
    <Folder name="e2e" />
    <Folder name="fixtures" />
    <Folder name="helpers" />
  </Folder>
</FileTree>

### Exemples de tests

**Test unitaire** (`tests/unit/example.test.ts`) :
```typescript
describe('Example Test Suite', () => {
  it('should pass this example test', () => {
    expect(true).toBe(true);
  });

  it('should test basic math', () => {
    expect(2 + 2).toBe(4);
  });
});
```

**Test de composants** (`tests/components/Button.test.tsx`) :
```typescript
import { render, screen, fireEvent } from '@testing-library/react';
import Button from '@/components/Button';

describe('Button Component', () => {
  it('renders with text', () => {
    render(<Button>Click Me</Button>);
    expect(screen.getByText('Click Me')).toBeInTheDocument();
  });

  it('calls onClick when clicked', () => {
    const handleClick = jest.fn();
    render(<Button onClick={handleClick}>Click</Button>);
    fireEvent.click(screen.getByText('Click'));
    expect(handleClick).toHaveBeenCalledTimes(1);
  });
});
```

**Test d'intégration** (`tests/integration/api.test.ts`) :
```typescript
import request from 'supertest';
import app from '@/app';

describe('API Integration Tests', () => {
  it('GET / should return 200', async () => {
    const response = await request(app).get('/');
    expect(response.status).toBe(200);
  });

  it('POST /api/users should create user', async () => {
    const response = await request(app)
      .post('/api/users')
      .send({ name: 'Test User', email: 'test@example.com' });
    expect(response.status).toBe(201);
    expect(response.body).toHaveProperty('id');
  });
});
```

**Test E2E** (`tests/e2e/login.spec.ts`) :
```typescript
import { test, expect } from '@playwright/test';

test('user can log in', async ({ page }) => {
  await page.goto('http://localhost:3000/login');
  await page.fill('input[name="email"]', 'test@example.com');
  await page.fill('input[name="password"]', 'password123');
  await page.click('button[type="submit"]');
  await expect(page).toHaveURL('http://localhost:3000/dashboard');
});
```

### Scripts NPM

`package.json` mis à jour :
```json
{
  "scripts": {
    "test": "jest",
    "test:watch": "jest --watch",
    "test:coverage": "jest --coverage",
    "test:unit": "jest tests/unit",
    "test:integration": "jest tests/integration",
    "test:e2e": "playwright test"
  }
}
```

### Intégration CI

Ajoute une tâche de test à `.github/workflows/ci.yml` :
```yaml
test:
  runs-on: ubuntu-latest
  steps:
    - uses: actions/checkout@v4

    - name: Setup Node
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'

    - name: Install dependencies
      run: npm ci

    - name: Run tests
      run: npm test -- --coverage

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage/lcov.info
```

### Documenttation

Crée `docs/02-practices/testing.md` :
```markdown
# Testing Guide

## Running Tests

npm test              # Run all tests
npm run test:watch    # Watch mode
npm run test:coverage # With coverage report
npm run test:unit     # Unit tests only

## Writing Tests

### Unit Tests
- Test individual functions/classes in isolation
- Mock external dependencies
- Fast (<10ms per test)

### Integration Tests
- Test multiple components together
- Use real dependencies when possible
- Medium speed (<100ms per test)

### E2E Tests
- Test full user flows
- Run against real app
- Slow (seconds per test)

## Coverage Requirements

- Minimum 70% coverage (enforced in CI)
- New code should be 90%+ covered
- Critical paths require 100% coverage

## Best Practices

- Use descriptive test names (Given/When/Then)
- One assertion per test when possible
- Avoid test interdependence
- Use factories/fixtures for test data
```

##Types de tests

### Tests unitaires
**Quoi** : tester des fonctions/classes individuelles de manière isolée
**Vitesse** : rapide (< 10 ms chacun)
**Moqueur** : Oui, fausses dépendances
**Couverture** : la plupart des codes doivent comporter des tests unitaires

Exemple : Testez une fonction utilitaire qui calcule l'âge de l'utilisateur.

### Tests d'intégration
**Quoi** : tester plusieurs composants fonctionnant ensemble
**Vitesse** : moyenne (< 100 ms chacune)
**Mocking** : minimal, utilisez de vrais composants
**Couverture** : workflows critiques et routes API

Exemple : testez que le flux d'authentification fonctionne de bout en bout avec la base de données.

### Tests E2E (facultatif)
**Quoi** : Testez des flux d'utilisateurs complets dans une application réelle
**Vitesse** : lente (secondes par test)
**Moqueur** : Aucun, application réelle
**Couverture** : parcours utilisateur critiques uniquement

Exemple : inscription de l'utilisateur → connexion → affichage du flux du tableau de bord.

## Seuils de couverture

Seuils par défaut (raisonnables, pas perfectionnistes) :
- **Branches** : 70 % - Tous les chemins de décision couverts
- **Fonctions** : 70 % - Toutes les fonctions appellentéd
- **Lignes** : 70 % - Toutes les lignes exécutées
- **Relevés** : 70 % - Toutes les déclarations sont exécutées

**Pourquoi pas 100 % ?**
- Une couverture à 100 % ne garantit pas une exactitude à 100 %
- Certains codes sont difficiles à tester (UI, cas extrêmes)
- Rendements décroissants après 70-80%
- Concentrez-vous plutôt sur les chemins critiques

## Exécution de tests

### Pendant le développement
```bash
npm run test:watch
```
Exécute automatiquement des tests lorsque les fichiers changent.

### Avant la validation
```bash
npm test
npm run test:coverage
```
Suite de tests complète avec vérification de couverture.

### Dans CI
S'exécute automatiquement à chaque poussée :
```bash
npm test -- --coverage --ci
```

## Flux de travail

La configuration suit ces étapes :

1. **Détecter la langue/l'exécution**
   - Recherche package.json, Gemfile, go.mod, etc.
   - Détermine le cadre approprié

2. **Vérifiez la configuration existante**
   - Analyse les répertoires de tests (test/, tests/, __tests__/)
   - Vérifie les fichiers de configuration de test
   - Détecte la configuration CI

3. **Afficher le plan d'installation**
   ```
   Will install:
   - jest, @types/jest, ts-jest
   - @testing-library/react

   Will create:
   - jest.config.js
   - tests/ directory structure
   - Example tests

   Will update:
   - package.json (test scripts)
   - .github/workflows/ci.yml (test job)

   Proceed? (YES/NO)
   ```

4. **Installer les dépendances**
   - Installation npm, installation pip, installation bundle, etc.

5. **Créer une configuration**
   - Spécifique au frameworkfichiers de configuration c

6. **Générer des exemples**
   - Exemple de test unitaire
   - Exemple de test d'intégration
   - Exemple de test E2E (si demandé)

7. **Mettre à jour les scripts et CI**
   - Ajouter des commandes de test
   - Intégration au flux de travail existant

8. **Exécuter des tests**
   - Vérifier que la configuration fonctionne
   - Afficher le rapport de couverture

9. **Créer une documentation**
   - Guide de test
   - Bonnes pratiques
   - Attentes de couverture

## Prochaines étapes

Une fois la configuration terminée :

1. **Exécuter des tests localement**
   ```bash
   npm test
   npm run test:watch
   ```

2. **Écrire les premiers tests**
   - Commencez par des tests unitaires pour les utilitaires
   - Ajouter des tests d'intégration pour les API
   - Ajouter ultérieurement des tests E2E pour les flux critiques

3. **Activer les vérifications de couverture**
   - CI nécessite une couverture minimale
   - Fusion bloquée si la couverture diminue

4. **Surveiller les tendances de couverture**
   - Utilisez codecov.io ou similaire
   - Suivre la couverture au fil du temps
   - Célébrez les améliorations

## meilleures pratiques

1. **Écrire les tests tôt** – TDD ou fonctionnalités parallèles
2. **Gardez les tests rapides** – simulez des opérations lentes
3. **Utiliser des noms descriptifs** - Nom du teste explique ce qu'il teste
4. **Une assertion par test** – Si possible, indique clairement les échecs
5. **DRY in Tests** - Utiliser les montages, les usines, la configuration/démontage
6. **Comportement du test** – Pas de détails d'implémentation
7. **Évitez la flakiness** - Pas de problèmes de timing, de pannes aléatoires
8. **Clean Up** - Réinitialiser l'état entre les tests

## Commandes associées

- [`/ci-setup`](/commands/ci) - Configurer le flux de travail CI/CD
- [`/setup-deployment`](/commands/deploy) - Configurer le déploiement
- [`/packages`](/commands/packages) - Gérer les dépendances des tests